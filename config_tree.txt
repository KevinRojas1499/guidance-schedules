CONFIG
├── mode
│   └── ppl_eval                                                                                                                          
├── diffusion
│   └── uniform                                                                                                                           
├── backbone
│   └── hf_dit                                                                                                                            
├── classifier_backbone
│   └── None                                                                                                                              
├── parameterization
│   └── d3pm                                                                                                                              
├── time_conditioning
│   └── True                                                                                                                              
├── subs_masking
│   └── False                                                                                                                             
├── zero_recon_loss
│   └── True                                                                                                                              
├── T
│   └── 0                                                                                                                                 
├── is_vision
│   └── False                                                                                                                             
├── seed
│   └── 1                                                                                                                                 
├── loader
│   └── global_batch_size: 512                                                                                                            
│       eval_global_batch_size: 512                                                                                                       
│       batch_size: 64                                                                                                                    
│       eval_batch_size: 64                                                                                                               
│       num_workers: 64                                                                                                                   
│       pin_memory: true                                                                                                                  
│       persistent_workers: true                                                                                                          
│                                                                                                                                         
├── sampling
│   └── use_cache: true                                                                                                                   
│       steps: 128                                                                                                                        
│       batch_size: 64                                                                                                                    
│       num_sample_batches: 2                                                                                                             
│       use_float64: false                                                                                                                
│                                                                                                                                         
├── training
│   └── ema: 0.9999                                                                                                                       
│       antithetic_sampling: true                                                                                                         
│       importance_sampling: false                                                                                                        
│       sampling_eps: 0.001                                                                                                               
│       change_of_variables: false                                                                                                        
│       compute_loss_on_pad_tokens: false                                                                                                 
│       use_simple_ce_loss: false                                                                                                         
│       guidance: null                                                                                                                    
│                                                                                                                                         
├── eval
│   └── checkpoint_path: ''                                                                                                               
│       disable_ema: false                                                                                                                
│       generate_samples: true                                                                                                            
│       generated_samples_path: ''                                                                                                        
│       max_samples: 50000                                                                                                                
│                                                                                                                                         
├── optim
│   └── weight_decay: 0                                                                                                                   
│       lr: 0.0003                                                                                                                        
│       beta1: 0.9                                                                                                                        
│       beta2: 0.999                                                                                                                      
│       eps: 1.0e-08                                                                                                                      
│                                                                                                                                         
├── trainer
│   └── _target_: lightning.Trainer                                                                                                       
│       accelerator: cuda                                                                                                                 
│       num_nodes: 1                                                                                                                      
│       devices: 8                                                                                                                        
│       accumulate_grad_batches: 1                                                                                                        
│       gradient_clip_val: 1.0                                                                                                            
│       precision: bf16                                                                                                                   
│       num_sanity_val_steps: 2                                                                                                           
│       max_steps: 1000000                                                                                                                
│       log_every_n_steps: 10                                                                                                             
│       limit_train_batches: 1.0                                                                                                          
│       limit_val_batches: 1.0                                                                                                            
│       val_check_interval: 10000                                                                                                         
│                                                                                                                                         
├── wandb
│   └── project: discrete-diffusion-guidance                                                                                              
│       notes: Guided Discrete Diffusion                                                                                                  
│       group: null                                                                                                                       
│       job_type: training                                                                                                                
│       name: null                                                                                                                        
│       id: None_1                                                                                                                        
│       tags:                                                                                                                             
│       - loglinear                                                                                                                       
│       - lm1b                                                                                                                            
│       - lm1b                                                                                                                            
│                                                                                                                                         
├── checkpointing
│   └── save_dir: /network/rit/lab/Yelab/kevin-back/kevin_rojas/repos/discrete-diffusion-guidance                                         
│       resume_from_ckpt: true                                                                                                            
│       resume_ckpt_path: /network/rit/lab/Yelab/kevin-back/kevin_rojas/repos/discrete-diffusion-guidance/checkpoints/last.ckpt           
│                                                                                                                                         
├── callbacks
│   └── checkpoint_every_n_steps:                                                                                                         
│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                                                                           
│         save_top_k: -1                                                                                                                  
│         save_last: true                                                                                                                 
│         dirpath: /network/rit/lab/Yelab/kevin-back/kevin_rojas/repos/discrete-diffusion-guidance/checkpoints                            
│         verbose: true                                                                                                                   
│         auto_insert_metric_name: false                                                                                                  
│         every_n_train_steps: 500                                                                                                        
│       checkpoint_monitor:                                                                                                               
│         _target_: lightning.pytorch.callbacks.ModelCheckpoint                                                                           
│         monitor: val/nll                                                                                                                
│         mode: min                                                                                                                       
│         save_top_k: 1                                                                                                                   
│         save_last: false                                                                                                                
│         dirpath: /network/rit/lab/Yelab/kevin-back/kevin_rojas/repos/discrete-diffusion-guidance/checkpoints                            
│         filename: best                                                                                                                  
│         auto_insert_metric_name: false                                                                                                  
│         verbose: true                                                                                                                   
│       learning_rate_monitor:                                                                                                            
│         _target_: lightning.pytorch.callbacks.LearningRateMonitor                                                                       
│         logging_interval: step                                                                                                          
│                                                                                                                                         
├── data
│   └── train: lm1b                                                                                                                       
│       valid: lm1b                                                                                                                       
│       tokenizer_name_or_path: bert-base-uncased                                                                                         
│       cache_dir: ./cache/lm1b                                                                                                           
│       wrap: false                                                                                                                       
│       streaming: false                                                                                                                  
│       override_cache: false                                                                                                             
│       add_special_tokens: true                                                                                                          
│                                                                                                                                         
├── model
│   └── pretrained_model_name_or_path: kuleshov-group/udlm-lm1b                                                                           
│       length: 128                                                                                                                       
│                                                                                                                                         
├── strategy
│   └── _target_: lightning.pytorch.strategies.DDPStrategy                                                                                
│       find_unused_parameters: false                                                                                                     
│                                                                                                                                         
├── noise
│   └── type: loglinear                                                                                                                   
│       sigma_min: 0.0001                                                                                                                 
│       sigma_max: 20                                                                                                                     
│                                                                                                                                         
└── lr_scheduler
    └── _target_: transformers.get_constant_schedule_with_warmup                                                                          
        num_warmup_steps: 2500                                                                                                            
                                                                                                                                          
